\cvsection{Selected Publications}
\vspace{-1.0mm}

\cvsubsection{Efficient Transformers \& Distillation}
\begin{cventries}
  \cventry
    {\textbf{Cenk Baykal}, Dylan Cutler, Nishanth Dikkala, Nikhil Ghosh, Rina Panigrahy, Xin Wang}
    {Alternating Updates for Efficient Transformers \color{red}{(Spotlight)}}
    {NeurIPS}
    {2023}
    {\begin{cvitems}
        \item {Introduced alternating forward/backward scheduling that reduced inference latency for sparse transformers; Google Scholar: 3 citations (accessed September 21, 2025).}
      \end{cvitems}}

  \cventry
    {Vasilis Kontonis, Fotis Iliopoulos, Khoa Trinh, \textbf{Cenk Baykal}, Gaurav Menghani, Erik Vee}
    {SLAM: Student-label Mixing for Distillation with Unlabeled Examples}
    {NeurIPS}
    {2023}
    {\begin{cvitems}
        \item {Demonstrated unlabeled-target distillation with hybrid pseudo-labeling that cut annotation costs in half; Google Scholar: 7 citations (accessed September 21, 2025).}
      \end{cvitems}}

  \cventry
    {\textbf{Cenk Baykal}, Khoa Trinh, Fotis Iliopoulos, Gaurav Menghani, Erik Vee}
    {Robust Active Distillation}
    {ICLR}
    {2023}
    {\begin{cvitems}
        \item {Coupled active learning with knowledge distillation to improve compact model robustness against label noise; Google Scholar: 11 citations (accessed September 21, 2025).}
      \end{cvitems}}

  \cventry
    {Vasilis Kontonis, Fotis Iliopoulos, \textbf{Cenk Baykal}, Gaurav Menghani, Khoa Trinh, Erik Vee}
    {Weighted Distillation with Unlabeled Examples}
    {NeurIPS}
    {2022}
    {\begin{cvitems}
        \item {Introduced importance-weighted student objectives for unlabeled corpora, yielding state-of-the-art compact transformers; Google Scholar: 13 citations (accessed September 21, 2025).}
      \end{cvitems}}
\end{cventries}

\cvsubsection{Model Compression \& Pruning}
\begin{cventries}
  \cventry
    {\textbf{Cenk Baykal}*, Lucas Liebenwein*, Igor Gilitschenski, Dan Feldman, Daniela Rus}
    {SiPPing Neural Networks: Sensitivity-informed Provable Pruning of Neural Networks}
    {SIAM SIMODS}
    {2022}
    {\begin{cvitems}
        \item {Designed a sensitivity-aware pruning scheme with provable bounds that retained accuracy on language models; Google Scholar: 32 citations (accessed September 21, 2025).}
      \end{cvitems}}

  \cventry
    {Lucas Liebenwein*, \textbf{Cenk Baykal}*, Harry Lang, Dan Feldman, Daniela Rus}
    {Provable Filter Pruning for Efficient Neural Networks}
    {ICLR}
    {2020}
    {\begin{cvitems}
        \item {Provided the first end-to-end guarantees for structural CNN pruning, inspiring broad adoption in efficient CNN research; Google Scholar: 190 citations (accessed September 21, 2025).}
      \end{cvitems}}

  \cventry
    {Lucas Liebenwein, \textbf{Cenk Baykal}, Brandon Carter, David Gifford, Daniela Rus}
    {Lost in Pruning: The Effects of Pruning Neural Networks beyond Test Accuracy}
    {MLSys}
    {2021}
    {\begin{cvitems}
        \item {Analyzed deployment-side regressions (latency, calibration) caused by aggressive pruning, motivating safer evaluation metrics; Google Scholar: 82 citations (accessed September 21, 2025).}
      \end{cvitems}}

  \cventry
    {\textbf{Cenk Baykal}*, Lucas Liebenwein*, Igor Gilitschenski, Dan Feldman, Daniela Rus}
    {Data-Dependent Coresets for Compressing Neural Networks with Applications to Generalization Bounds}
    {ICLR}
    {2019}
    {\begin{cvitems}
        \item {Developed dataset-aware coreset selection that yields compression with theoretical generalization guarantees; Google Scholar: 93 citations (accessed September 21, 2025).}
      \end{cvitems}}
\end{cventries}

\cvsubsection{Coresets \& Theory}
\begin{cventries}
  \cventry
    {\textbf{Cenk Baykal}*, Murad Tukan*, Dan Feldman, Daniela Rus}
    {Coresets for Support Vector Machines}
    {Theory of Computing Systems}
    {2021}
    {\begin{cvitems}
        \item {Unified the streaming and distributed coreset constructions for SVMs; Google Scholar: 48 citations (accessed September 21, 2025). Extended version of the TAMC 2020 oral paper.}
      \end{cvitems}}
\end{cventries}
